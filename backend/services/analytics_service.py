#!/usr/bin/env python3
"""
AHAII Analytics Service
Comprehensive metrics compilation and dashboard analytics for the AHAII system
Tracks data collection progress, sources, domains, and system performance
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict

from loguru import logger

from .database_service import DatabaseService
from config.database import supabase


@dataclass
class DataCollectionMetrics:
    """Comprehensive data collection metrics"""
    # Core data counts
    total_countries: int = 0
    total_infrastructure_indicators: int = 0
    total_health_ai_organizations: int = 0
    total_infrastructure_intelligence: int = 0
    total_ahaii_assessments: int = 0
    total_academic_papers: int = 0
    
    # Data collection activity
    records_collected_last_24h: int = 0
    records_collected_last_7d: int = 0
    records_collected_last_30d: int = 0
    
    # Data source breakdown
    source_distribution: Dict[str, int] = None
    domain_distribution: Dict[str, int] = None
    country_coverage: Dict[str, int] = None
    
    # Data quality metrics
    verified_records: int = 0
    high_confidence_records: int = 0
    peer_reviewed_sources: int = 0
    
    # System performance
    uptime_days: float = 0.0
    last_etl_run: Optional[datetime] = None
    successful_pipeline_runs: int = 0
    failed_pipeline_runs: int = 0
    
    # Specialized metrics
    snowball_discoveries: int = 0
    government_docs_processed: int = 0
    citation_networks_mapped: int = 0
    african_relevance_avg: float = 0.0
    ai_relevance_avg: float = 0.0
    
    # Temporal metrics
    collection_start_date: Optional[datetime] = None
    days_operational: int = 0
    avg_records_per_day: float = 0.0
    
    def __post_init__(self):
        if self.source_distribution is None:
            self.source_distribution = {}
        if self.domain_distribution is None:
            self.domain_distribution = {}
        if self.country_coverage is None:
            self.country_coverage = {}


class AHAIIAnalyticsService:
    """Comprehensive analytics service for AHAII dashboard metrics"""
    
    def __init__(self):
        self.db_service = DatabaseService()
    
    async def get_comprehensive_metrics(self) -> DataCollectionMetrics:
        """Get all metrics for the dashboard in one comprehensive call"""
        logger.info("üîç Compiling comprehensive AHAII analytics...")
        
        metrics = DataCollectionMetrics()
        
        try:
            # Run all metric collection tasks in parallel for efficiency
            await asyncio.gather(
                self._collect_core_counts(metrics),
                self._collect_activity_metrics(metrics),
                self._collect_source_distributions(metrics),
                self._collect_quality_metrics(metrics),
                self._collect_system_performance(metrics),
                self._collect_specialized_metrics(metrics),
                self._collect_temporal_metrics(metrics)
            )
            
            logger.info(f"‚úÖ Analytics compilation complete: {metrics.total_infrastructure_intelligence} intelligence records tracked")
            return metrics
            
        except Exception as e:
            logger.error(f"‚ùå Analytics compilation failed: {e}")
            return metrics  # Return partial metrics rather than failing completely
    
    async def _collect_core_counts(self, metrics: DataCollectionMetrics):
        """Collect core data table counts"""
        try:
            # Countries
            countries_result = await supabase.table("countries").select("id").execute()
            metrics.total_countries = len(countries_result.data) if countries_result.data else 0
            
            # Infrastructure indicators
            indicators_result = await supabase.table("infrastructure_indicators").select("id").execute()
            metrics.total_infrastructure_indicators = len(indicators_result.data) if indicators_result.data else 0
            
            # Health AI organizations
            orgs_result = await supabase.table("health_ai_organizations").select("id").execute()
            metrics.total_health_ai_organizations = len(orgs_result.data) if orgs_result.data else 0
            
            # Infrastructure intelligence (main data store)
            intelligence_result = await supabase.table("infrastructure_intelligence").select("id").execute()
            metrics.total_infrastructure_intelligence = len(intelligence_result.data) if intelligence_result.data else 0
            
            # AHAII assessments/scores
            scores_result = await supabase.table("ahaii_scores").select("id").execute()
            metrics.total_ahaii_assessments = len(scores_result.data) if scores_result.data else 0
            
            # Academic papers (from intelligence with academic source types)
            academic_result = await supabase.table("infrastructure_intelligence").select("id").in_("source_type", ["academic_paper", "systematic_review", "preprint"]).execute()
            metrics.total_academic_papers = len(academic_result.data) if academic_result.data else 0
            
        except Exception as e:
            logger.warning(f"Failed to collect core counts: {e}")
    
    async def _collect_activity_metrics(self, metrics: DataCollectionMetrics):
        """Collect recent data collection activity"""
        try:
            now = datetime.now()
            
            # Last 24 hours
            last_24h = now - timedelta(hours=24)
            recent_result = await supabase.table("infrastructure_intelligence").select("id").gte("created_at", last_24h.isoformat()).execute()
            metrics.records_collected_last_24h = len(recent_result.data) if recent_result.data else 0
            
            # Last 7 days  
            last_7d = now - timedelta(days=7)
            week_result = await supabase.table("infrastructure_intelligence").select("id").gte("created_at", last_7d.isoformat()).execute()
            metrics.records_collected_last_7d = len(week_result.data) if week_result.data else 0
            
            # Last 30 days
            last_30d = now - timedelta(days=30)
            month_result = await supabase.table("infrastructure_intelligence").select("id").gte("created_at", last_30d.isoformat()).execute()
            metrics.records_collected_last_30d = len(month_result.data) if month_result.data else 0
            
        except Exception as e:
            logger.warning(f"Failed to collect activity metrics: {e}")
    
    async def _collect_source_distributions(self, metrics: DataCollectionMetrics):
        """Collect data source and domain distributions"""
        try:
            # Source types distribution
            sources_result = await supabase.table("infrastructure_intelligence").select("source_type").execute()
            if sources_result.data:
                source_counts = defaultdict(int)
                domain_counts = defaultdict(int)
                
                for record in sources_result.data:
                    source_type = record.get("source_type", "unknown")
                    source_counts[source_type] += 1
                
                metrics.source_distribution = dict(source_counts)
            
            # Domain distribution from URLs
            urls_result = await supabase.table("infrastructure_intelligence").select("source_url").execute()
            if urls_result.data:
                domain_counts = defaultdict(int)
                
                for record in urls_result.data:
                    url = record.get("source_url", "")
                    if url:
                        # Extract domain from URL
                        try:
                            from urllib.parse import urlparse
                            domain = urlparse(url).netloc
                            if domain:
                                # Clean up domain (remove www., etc.)
                                domain = domain.lower().replace('www.', '')
                                domain_counts[domain] += 1
                        except:
                            continue
                
                # Keep only top 20 domains to avoid clutter
                sorted_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:20]
                metrics.domain_distribution = dict(sorted_domains)
            
            # Country coverage
            country_result = await supabase.table("infrastructure_intelligence").select("country_id, countries!inner(name)").execute()
            if country_result.data:
                country_counts = defaultdict(int)
                
                for record in country_result.data:
                    if record.get("countries"):
                        country_name = record["countries"]["name"]
                        country_counts[country_name] += 1
                
                metrics.country_coverage = dict(country_counts)
            
        except Exception as e:
            logger.warning(f"Failed to collect source distributions: {e}")
    
    async def _collect_quality_metrics(self, metrics: DataCollectionMetrics):
        """Collect data quality metrics"""
        try:
            # Verified records
            verified_result = await supabase.table("infrastructure_intelligence").select("id").eq("verification_status", "verified").execute()
            metrics.verified_records = len(verified_result.data) if verified_result.data else 0
            
            # High confidence records (confidence_score > 0.7)
            high_conf_result = await supabase.table("infrastructure_intelligence").select("id").gt("confidence_score", 0.7).execute()
            metrics.high_confidence_records = len(high_conf_result.data) if high_conf_result.data else 0
            
            # Peer reviewed sources
            peer_reviewed_result = await supabase.table("infrastructure_intelligence").select("id").eq("verification_status", "peer_reviewed").execute()
            metrics.peer_reviewed_sources = len(peer_reviewed_result.data) if peer_reviewed_result.data else 0
            
        except Exception as e:
            logger.warning(f"Failed to collect quality metrics: {e}")
    
    async def _collect_system_performance(self, metrics: DataCollectionMetrics):
        """Collect system performance metrics"""
        try:
            # This would typically come from system monitoring
            # For now, we'll calculate based on available data
            
            # Find earliest record to calculate uptime
            earliest_result = await supabase.table("infrastructure_intelligence").select("created_at").order("created_at").limit(1).execute()
            if earliest_result.data:
                earliest_date = datetime.fromisoformat(earliest_result.data[0]["created_at"].replace('Z', '+00:00'))
                metrics.uptime_days = (datetime.now() - earliest_date.replace(tzinfo=None)).days
            
            # Last ETL run (most recent record)
            latest_result = await supabase.table("infrastructure_intelligence").select("created_at").order("created_at", desc=True).limit(1).execute()
            if latest_result.data:
                metrics.last_etl_run = datetime.fromisoformat(latest_result.data[0]["created_at"].replace('Z', '+00:00'))
            
            # Pipeline success/failure would typically come from logging system
            # For now, estimate based on data consistency
            metrics.successful_pipeline_runs = max(metrics.total_infrastructure_intelligence // 10, 1)  # Estimate
            metrics.failed_pipeline_runs = max(metrics.successful_pipeline_runs // 20, 0)  # Assume 5% failure rate
            
        except Exception as e:
            logger.warning(f"Failed to collect system performance: {e}")
    
    async def _collect_specialized_metrics(self, metrics: DataCollectionMetrics):
        """Collect specialized AHAII metrics"""
        try:
            # Snowball discoveries (records from citation extraction)
            snowball_result = await supabase.table("infrastructure_intelligence").select("id").ilike("report_type", "%snowball%").execute()
            metrics.snowball_discoveries = len(snowball_result.data) if snowball_result.data else 0
            
            # Government documents processed (conservative count)
            gov_domains = ["gov", "ministry", "department", "who.int", "afro.who.int"]
            gov_count = 0
            for domain in gov_domains:
                gov_result = await supabase.table("infrastructure_intelligence").select("id").ilike("source_url", f"%{domain}%").execute()
                if gov_result.data:
                    gov_count += len(gov_result.data)
            metrics.government_docs_processed = gov_count
            
            # Citation networks (estimate based on academic papers)
            metrics.citation_networks_mapped = metrics.total_academic_papers
            
            # Average relevance scores
            relevance_result = await supabase.table("infrastructure_intelligence").select("key_findings").execute()
            if relevance_result.data:
                african_scores = []
                ai_scores = []
                
                for record in relevance_result.data:
                    findings = record.get("key_findings", {})
                    if isinstance(findings, dict):
                        if "african_relevance_score" in findings:
                            try:
                                african_scores.append(float(findings["african_relevance_score"]))
                            except:
                                continue
                        if "ai_relevance_score" in findings or "health_ai_relevance_score" in findings:
                            try:
                                score = findings.get("ai_relevance_score") or findings.get("health_ai_relevance_score")
                                ai_scores.append(float(score))
                            except:
                                continue
                
                if african_scores:
                    metrics.african_relevance_avg = sum(african_scores) / len(african_scores)
                if ai_scores:
                    metrics.ai_relevance_avg = sum(ai_scores) / len(ai_scores)
            
        except Exception as e:
            logger.warning(f"Failed to collect specialized metrics: {e}")
    
    async def _collect_temporal_metrics(self, metrics: DataCollectionMetrics):
        """Collect temporal/historical metrics"""
        try:
            # Collection start date (earliest record)
            earliest_result = await supabase.table("infrastructure_intelligence").select("created_at").order("created_at").limit(1).execute()
            if earliest_result.data:
                metrics.collection_start_date = datetime.fromisoformat(earliest_result.data[0]["created_at"].replace('Z', '+00:00'))
                metrics.days_operational = (datetime.now() - metrics.collection_start_date.replace(tzinfo=None)).days
                
                # Average records per day
                if metrics.days_operational > 0:
                    metrics.avg_records_per_day = metrics.total_infrastructure_intelligence / metrics.days_operational
            
        except Exception as e:
            logger.warning(f"Failed to collect temporal metrics: {e}")
    
    async def get_dashboard_summary(self) -> Dict[str, Any]:
        """Get a concise dashboard summary for quick display"""
        metrics = await self.get_comprehensive_metrics()
        
        return {
            "headline_stats": {
                "total_records": metrics.total_infrastructure_intelligence,
                "countries_covered": metrics.total_countries,
                "days_operational": metrics.days_operational,
                "records_last_24h": metrics.records_collected_last_24h
            },
            "data_sources": {
                "academic_papers": metrics.total_academic_papers,
                "government_docs": metrics.government_docs_processed,
                "news_articles": metrics.source_distribution.get("news_article", 0),
                "total_sources": len(metrics.source_distribution)
            },
            "quality_metrics": {
                "verified_records": metrics.verified_records,
                "high_confidence": metrics.high_confidence_records,
                "peer_reviewed": metrics.peer_reviewed_sources,
                "avg_african_relevance": round(metrics.african_relevance_avg * 100, 1),
                "avg_ai_relevance": round(metrics.ai_relevance_avg * 100, 1)
            },
            "system_health": {
                "uptime_days": metrics.uptime_days,
                "successful_runs": metrics.successful_pipeline_runs,
                "last_update": metrics.last_etl_run.isoformat() if metrics.last_etl_run else None,
                "avg_daily_collection": round(metrics.avg_records_per_day, 1)
            },
            "top_domains": dict(list(metrics.domain_distribution.items())[:10]) if metrics.domain_distribution else {},
            "top_countries": dict(list(metrics.country_coverage.items())[:10]) if metrics.country_coverage else {}
        }
    
    async def get_time_series_data(self, days: int = 30) -> Dict[str, List[Dict]]:
        """Get time-series data for charts and trends"""
        try:
            now = datetime.now()
            start_date = now - timedelta(days=days)
            
            # Daily collection counts
            daily_data = []
            for i in range(days):
                day_start = start_date + timedelta(days=i)
                day_end = day_start + timedelta(days=1)
                
                day_result = await supabase.table("infrastructure_intelligence").select("id").gte("created_at", day_start.isoformat()).lt("created_at", day_end.isoformat()).execute()
                
                daily_data.append({
                    "date": day_start.strftime("%Y-%m-%d"),
                    "records": len(day_result.data) if day_result.data else 0
                })
            
            return {
                "daily_collection": daily_data
            }
        
        except Exception as e:
            logger.warning(f"Failed to collect time series data: {e}")
            return {"daily_collection": []}
    
    def to_dict(self, metrics: DataCollectionMetrics) -> Dict[str, Any]:
        """Convert metrics to dictionary for JSON serialization"""
        data = asdict(metrics)
        
        # Handle datetime serialization
        if data.get("last_etl_run"):
            data["last_etl_run"] = data["last_etl_run"].isoformat()
        if data.get("collection_start_date"):
            data["collection_start_date"] = data["collection_start_date"].isoformat()
        
        return data


async def get_dashboard_analytics():
    """Quick function to get analytics for dashboard"""
    service = AHAIIAnalyticsService()
    return await service.get_dashboard_summary()


async def get_full_analytics():
    """Get comprehensive analytics"""
    service = AHAIIAnalyticsService()
    metrics = await service.get_comprehensive_metrics()
    return service.to_dict(metrics)


if __name__ == "__main__":
    # CLI test
    import json
    
    async def test_analytics():
        service = AHAIIAnalyticsService()
        
        print("üîç Getting dashboard summary...")
        summary = await service.get_dashboard_summary()
        print(json.dumps(summary, indent=2, default=str))
        
        print("\nüìà Getting time series data...")
        time_series = await service.get_time_series_data(7)  # Last 7 days
        print(f"Time series points: {len(time_series['daily_collection'])}")
    
    asyncio.run(test_analytics())
